{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Index"
      ],
      "metadata": {
        "id": "XxytJnXYnTzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [Load the data](#7)\n",
        "1. [Preprocessing the data](#2)\n",
        "1. [GridSearch for different models](#6)\n",
        "    1. [Logistic Regression](#8)\n",
        "    1. [KNN](#9)\n",
        "    1. [Decision Tree](#10)\n",
        "    1. [Random Forest](#11)\n",
        "    1. [MLP](#11)\n",
        "1. [Comparison of the results](#6)\n",
        "1. [Test confusion matrix for the best model](#6)"
      ],
      "metadata": {
        "id": "RJ3Ug1NLncRV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IryZDDYK9Bzx"
      },
      "source": [
        "<a id=\"1\"></a>\n",
        "# 1. Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIVjr32H9DQW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'dataset1.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lbPdiWV9jqg"
      },
      "outputs": [],
      "source": [
        "data_cleaned = data.drop(columns=['Unnamed: 0'])  # Remove unnamed column\n",
        "\n",
        "column_names = data_cleaned.columns # Check if other columns make sense\n",
        "column_names"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of duplicate rows: {data.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "V8WH1GHeXUfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztuHHFZz9r20"
      },
      "outputs": [],
      "source": [
        "missing_values = data_cleaned.isnull().sum()\n",
        "\n",
        "missing_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVorLn4b9wAa"
      },
      "outputs": [],
      "source": [
        "summary_statistics = data_cleaned.describe()\n",
        "summary_statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZSTeVcM9zap"
      },
      "outputs": [],
      "source": [
        "data_cleaned_info = {\n",
        "    \"Number of Rows\": data_cleaned.shape[0],\n",
        "    \"Number of Columns\": data_cleaned.shape[1],\n",
        "    \"Column Names\": data_cleaned.columns.tolist(),\n",
        "    \"data_cleaned Types\": data_cleaned.dtypes.to_dict(),\n",
        "}\n",
        "\n",
        "data_cleaned_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rd5HkxTbfxBT"
      },
      "outputs": [],
      "source": [
        "target_distribution = data_cleaned['Target'].value_counts()\n",
        "target_distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preprocessing the data"
      ],
      "metadata": {
        "id": "b2jZDa_OgWHx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsC5CMrpAUsP"
      },
      "outputs": [],
      "source": [
        "# Split data into features (X) and target variable (y)\n",
        "X = data_cleaned.drop(columns=['Target'])\n",
        "y = data_cleaned['Target']\n",
        "\n",
        "# Encode target variable\n",
        "y = y.map({'R': 1, 'NR': 0})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into training and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Training data size: {X_train.shape}\")\n",
        "print(f\"Test data size: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "F7YVjO0EjUJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3m1jhrfAsuU"
      },
      "source": [
        "<a id=\"2\"></a>\n",
        "# 3. GridSearch for the different models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"9\"></a>\n",
        "## 3.1 Logistic Regression"
      ],
      "metadata": {
        "id": "2C0y7mNMnSqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import LeaveOneOut, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000, class_weight='balanced')\n",
        "\n",
        "# Define parameter grid for Logistic Regression\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 10],\n",
        "    'solver': ['sag' , 'liblinear']\n",
        "}\n",
        "\n",
        "# Leave-One-Out Cross-Validation (LOOCV)\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "# Perform Grid Search using LOOCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=loo, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best LOOCV score\n",
        "best_params_lr = grid_search.best_params_\n",
        "best_score_lr = grid_search.best_score_\n",
        "best_model_lr = grid_search.best_estimator_\n",
        "\n",
        "print(f\"Best Parameters: {best_params_lr}\")\n",
        "print(f\"Best accuracy score: {best_score_lr}\")\n"
      ],
      "metadata": {
        "id": "SA7ozdkMnPdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict probabilities\n",
        "y_train_prob = best_model_lr.predict_proba(X_train)[:, 1]  # Probabilities for the positive class\n",
        "\n",
        "# Plotting the probabilities\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(range(len(y_train)), y_train_prob, c=y_train, cmap='coolwarm', marker='o')\n",
        "plt.title('Logistic Regression Predicted Probabilities vs True Labels')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Predicted Probability')\n",
        "plt.colorbar(label='True Labels')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YAnOADvQx606"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "best_model_lr.fit(X_train, y_train)\n",
        "y_train_pred_lr = best_model_lr.predict(X_train)\n",
        "train_accuracy_lr = accuracy_score(y_train, y_train_pred_lr)\n",
        "\n",
        "print(\"Accuracy:\", train_accuracy_lr)\n",
        "print(classification_report(y_train, y_train_pred_lr))"
      ],
      "metadata": {
        "id": "73vcdHBRj5jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_test_pred_lr = best_model_lr.predict(X_test)\n",
        "test_accuracy_lr = accuracy_score(y_test, y_test_pred_lr)\n",
        "\n",
        "print(\"Accuracy:\", test_accuracy_lr)\n",
        "print(classification_report(y_test, y_test_pred_lr))"
      ],
      "metadata": {
        "id": "SZto2ewCj8_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_cm_lr = confusion_matrix(y_train, y_train_pred_lr)\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "sns.heatmap(train_cm_lr, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['NR', 'R'], yticklabels=['NR', 'R'])\n",
        "plt.title('Confusion Matrix for Training Set')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sHdVxq0M9qCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"9\"></a>\n",
        "## 3.2 KNN"
      ],
      "metadata": {
        "id": "BA4A6rIsocFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import LeaveOneOut, GridSearchCV\n",
        "\n",
        "# Create the KNN model\n",
        "model = KNeighborsClassifier()\n",
        "\n",
        "# Defining the hyperparameter search space\n",
        "param_grid = {\n",
        "    'n_neighbors': range(2, 11),\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
        "}\n",
        "\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=loo,scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params_knn = grid_search.best_params_\n",
        "best_score_knn= grid_search.best_score_\n",
        "best_model_knn= grid_search.best_estimator_\n",
        "\n",
        "print(f\"Best parameters: {best_params_knn}\")\n",
        "print(f\"Best accurancy score: {best_score_knn:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZWW1mIdXqrzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "best_model_knn.fit(X_train, y_train)\n",
        "y_train_pred_knn = best_model_knn.predict(X_train)\n",
        "train_accuracy_knn = accuracy_score(y_train, y_train_pred_knn)\n",
        "\n",
        "print(\"Accuracy:\", train_accuracy_knn)\n",
        "print(classification_report(y_train, y_train_pred_knn))"
      ],
      "metadata": {
        "id": "A7s09WF81LZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_test_pred_knn = best_model_knn.predict(X_test)\n",
        "test_accuracy_knn = accuracy_score(y_test, y_test_pred_knn)\n",
        "\n",
        "print(\"Accuracy:\", test_accuracy_knn)\n",
        "print(classification_report(y_test, y_test_pred_knn))"
      ],
      "metadata": {
        "id": "H9vzElXh-qHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_cm_knn = confusion_matrix(y_train, y_train_pred_knn)\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "sns.heatmap(train_cm_knn, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['NR', 'R'], yticklabels=['NR', 'R'])\n",
        "plt.title('Confusion Matrix for Training Set')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fmhQD3iNHZEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"9\"></a>\n",
        "## 3.3 Decision Tree"
      ],
      "metadata": {
        "id": "wPX5ujG_-vbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import LeaveOneOut, GridSearchCV\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "# Create the decision tree model\n",
        "model = DecisionTreeClassifier()\n",
        "\n",
        "# Defining the hyperparameter search space\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy', 'log_loss'],  # Quality function of division\n",
        "    'splitter': ['best', 'random'],               # Strategy for splitting nodes\n",
        "    'max_depth': [ 5, 10, 20],               # Maximum tree depth\n",
        "    'min_samples_split': [ 4, 7, 10],              # Minimum examples needed to split a node\n",
        "    'min_samples_leaf': [4,7,10],                # Minimum examples needed on a sheet\n",
        "}\n",
        "\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=loo, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params_dt = grid_search.best_params_\n",
        "best_score_dt= grid_search.best_score_\n",
        "best_model_dt=grid_search.best_estimator_\n",
        "\n",
        "print(f\"Mejores par√°metros: {best_params_dt}\")\n",
        "print(f\"Best accuracy score (LOOCV): {best_score_dt:.4f}\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(best_model_dt,\n",
        "          filled=True,\n",
        "          feature_names=X_train.columns.tolist(),\n",
        "          class_names=[str(cls) for cls in set(y_train)])\n",
        "plt.title(\"Decision Tree\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lariy121-vEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "best_model_dt.fit(X_train, y_train)\n",
        "y_train_pred_dt = best_model_dt.predict(X_train)\n",
        "train_accuracy_dt = accuracy_score(y_train, y_train_pred_dt)\n",
        "\n",
        "print(\"Accuracy:\", train_accuracy_dt)\n",
        "print(classification_report(y_train, y_train_pred_dt))"
      ],
      "metadata": {
        "id": "1jHrVSLmQw5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_test_pred_dt = best_model_dt.predict(X_test)\n",
        "test_accuracy_dt = accuracy_score(y_test, y_test_pred_dt)\n",
        "print(\"Accuracy:\", test_accuracy_dt)\n",
        "print(classification_report(y_test, y_test_pred_dt))"
      ],
      "metadata": {
        "id": "wUdIfDAsRSU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_cm_dt = confusion_matrix(y_train, y_train_pred_dt)\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "sns.heatmap(train_cm_dt, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['NR', 'R'], yticklabels=['NR', 'R'])\n",
        "plt.title('Confusion Matrix for Training Set')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NRdYIO-eScka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"9\"></a>\n",
        "## 3.4 Random Forest"
      ],
      "metadata": {
        "id": "1hVzHrHlSyQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import LeaveOneOut, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "\n",
        "# Create the RandomForest model\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Define the hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [5, 10, 15],  # Number of trees in the forest\n",
        "    'max_depth': [3, 5, 7],  # Maximum depth of the tree\n",
        "    'min_samples_split': [2, 4, 6],  # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [2, 4, 6],  # Minimum number of samples required to be at a leaf node\n",
        "    'max_features': ['sqrt', 'log2'],  # Number of features to consider for the best split\n",
        "    'bootstrap': [True]\n",
        "}\n",
        "\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=loo, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params_rf = grid_search.best_params_\n",
        "best_score_rf = grid_search.best_score_\n",
        "best_model_rf = grid_search.best_estimator_\n",
        "\n",
        "print(f\"Best parameters: {best_params_rf}\")\n",
        "print(f\"Best accuracy score: {best_score_rf:.4f}\")\n"
      ],
      "metadata": {
        "id": "yQaTEeKQS6zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "best_model_rf.fit(X_train, y_train)\n",
        "y_train_pred_rf = best_model_rf.predict(X_train)\n",
        "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
        "\n",
        "print(\"Accuracy:\", train_accuracy_rf)\n",
        "print(classification_report(y_train, y_train_pred_rf))"
      ],
      "metadata": {
        "id": "7rR5NuPeZP-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_test_pred_rf = best_model_rf.predict(X_test)\n",
        "test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n",
        "\n",
        "print(\"Accuracy:\", test_accuracy_rf)\n",
        "print(classification_report(y_test, y_test_pred_rf))"
      ],
      "metadata": {
        "id": "O65D19i0aEkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"9\"></a>\n",
        "## 3.5 MLP"
      ],
      "metadata": {
        "id": "GIkvicegh6eJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import LeaveOneOut, GridSearchCV\n",
        "\n",
        "# Define the Multilayer Perceptron model first\n",
        "mlp_model = MLPClassifier()\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(5,), (10,), (15,), (20,)],  # Single and multi-layer configurations\n",
        "    'activation': ['relu', 'tanh', 'logistic'],  # Activation functions\n",
        "    'solver': ['adam'],  # Weight optimization\n",
        "    'learning_rate': ['constant', 'adaptive'],  # Learning rate schedule\n",
        "    'alpha': [0.0001, 0.001],  # Regularization parameter\n",
        "}\n",
        "\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "grid_search = GridSearchCV(estimator = mlp_model, param_grid = param_grid, cv = loo, scoring = 'accuracy', n_jobs = -1, verbose = 1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params_mlp = grid_search.best_params_\n",
        "best_score_mlp = grid_search.best_score_\n",
        "best_model_mlp = grid_search.best_estimator_\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params_mlp)\n",
        "print(\"Best Cross-validated Accuracy:\", best_score_mlp)"
      ],
      "metadata": {
        "id": "UF64JWWph_4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "best_model_mlp.fit(X_train, y_train)\n",
        "y_train_pred_mlp = best_model_mlp.predict(X_train)\n",
        "train_accuracy_mlp = accuracy_score(y_train, y_train_pred_mlp)\n",
        "\n",
        "print(\"Accuracy:\", train_accuracy_mlp)\n",
        "print(classification_report(y_train, y_train_pred_mlp))"
      ],
      "metadata": {
        "id": "xeodzr7dXRcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_test_pred_mlp = best_model_mlp.predict(X_test)\n",
        "test_accuracy_mlp = accuracy_score(y_test, y_test_pred_mlp)\n",
        "\n",
        "print(\"Accuracy:\", test_accuracy_mlp)\n",
        "print(classification_report(y_test, y_test_pred_mlp))"
      ],
      "metadata": {
        "id": "p6RnLJEcYUER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Comparision of the results"
      ],
      "metadata": {
        "id": "Xp-u7D9ZgjOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Define the LeaveOneOut cross-validation object\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "# List of models, their predictions, and their training accuracies\n",
        "models = [\n",
        "    ('Logistic Regression', best_model_lr, y_test_pred_lr, best_params_lr, train_accuracy_lr),\n",
        "    ('Decision Trees', best_model_dt, y_test_pred_dt, best_params_dt, train_accuracy_dt),\n",
        "    ('KNN', best_model_knn, y_test_pred_knn, best_params_knn, train_accuracy_knn),\n",
        "    ('Random Forest', best_model_rf, y_test_pred_rf, best_params_rf, train_accuracy_rf),\n",
        "    ('MLP', best_model_mlp, y_test_pred_mlp, best_params_mlp, train_accuracy_mlp)\n",
        "]\n",
        "\n",
        "# List to store the results\n",
        "results = []\n",
        "\n",
        "# Iterate through the models and calculate the metrics\n",
        "for model_name, model, y_test_pred, best_params, train_accuracy in models:\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    f1 = f1_score(y_test, y_test_pred)  # F1 Score\n",
        "    cv_score = cross_val_score(model, X_train, y_train, cv=loo).mean()  # Cross-validation score\n",
        "    validation_error = 1 - cv_score  # Validation error is the complement of CV score\n",
        "\n",
        "    # Append results to the list\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'Best Parameters': best_params,\n",
        "        'Train Accuracy': train_accuracy,\n",
        "        'Test Accuracy': test_accuracy,\n",
        "        'F1 Score': f1,\n",
        "        'CV Score': cv_score,\n",
        "        'Validation Error': validation_error\n",
        "    })\n",
        "\n",
        "# Convert the results to a pandas DataFrame for better readability\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Create a bar plot with separate bars for train accuracy, test accuracy, f1 score, cv score, and validation error\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Set bar positions for each metric\n",
        "train_positions = range(len(results_df))\n",
        "test_positions = [x + 0.2 for x in train_positions]  # Shift the test bars to the right\n",
        "f1_positions = [x + 0.4 for x in train_positions]    # Shift the f1 score bars further right\n",
        "cv_positions = [x + 0.6 for x in train_positions]    # Shift the cv score bars further right\n",
        "validation_error_positions = [x + 0.8 for x in train_positions]  # Shift the Validation Error bars further right\n",
        "\n",
        "# Plotting the bars\n",
        "ax.bar(train_positions, results_df['Train Accuracy'], width=0.16, label='Train Accuracy', color='purple')\n",
        "ax.bar(test_positions, results_df['Test Accuracy'], width=0.16, label='Test Accuracy', color='salmon')\n",
        "ax.bar(f1_positions, results_df['F1 Score'], width=0.16, label='F1 Score', color='skyblue')\n",
        "ax.bar(cv_positions, results_df['CV Score'], width=0.16, label='CV Score', color='orange')\n",
        "ax.bar(validation_error_positions, results_df['Validation Error'], width=0.16, label='Validation Error', color='green')\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('Models')\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Comparison of Train, Test Accuracy, F1 Score, CV Score, and Validation Error for Each Model')\n",
        "ax.set_xticks([x + 0.4 for x in train_positions])  # Set x-axis ticks at the center of the grouped bars\n",
        "ax.set_xticklabels(results_df['Model'])\n",
        "\n",
        "# Adding a legend\n",
        "ax.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display the results in a DataFrame\n",
        "results_df\n"
      ],
      "metadata": {
        "id": "lxgQEYmjP_KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Test confusion matrix for the best model"
      ],
      "metadata": {
        "id": "caKdh02tO1YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "test_cm_knn = confusion_matrix(y_test, y_test_pred_knn)\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "sns.heatmap(test_cm_knn, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['NR', 'R'], yticklabels=['NR', 'R'])\n",
        "plt.title('Confusion Matrix for Testing Set')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mSDJsnCsOxge"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}